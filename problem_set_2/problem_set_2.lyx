#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\renewcommand{\labelenumi}{\alph{enumi})}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Kernel ridge regression
\end_layout

\begin_layout Enumerate
Taking partial derivates, we get 
\begin_inset Formula 
\[
\frac{\partial J(\theta)}{\partial\theta_{j}}=\sum_{i=1}^{m}(\theta^{T}x^{(i)}-y^{(i)})x_{j}^{(i)}+\lambda\theta_{j}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Writing in matrix notation and setting to zero, this is
\begin_inset Formula 
\[
X^{T}(X\theta-Y)+\lambda I\theta=0.
\]

\end_inset


\end_layout

\begin_layout Standard
Solving for 
\begin_inset Formula $\theta$
\end_inset

, we get 
\begin_inset Formula 
\[
\theta=(X^{T}X+\lambda I)^{-1}X^{T}Y.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $\Phi$
\end_inset

 be the matrix we get by applying 
\begin_inset Formula $\phi$
\end_inset

 to X row-wise.
 That is, the 
\begin_inset Formula $i$
\end_inset

th row of 
\begin_inset Formula $\Phi$
\end_inset

 is 
\begin_inset Formula $\phi(x^{(i)})$
\end_inset

.
 Using the hint, we can rewrite 
\begin_inset Formula $\theta$
\end_inset

 as 
\begin_inset Formula 
\[
\theta=\Phi^{T}(\lambda I+\Phi\Phi^{T})^{-1}Y.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The 
\begin_inset Formula $i,j$
\end_inset

th entry of 
\begin_inset Formula $\Phi\Phi^{T}$
\end_inset

 is 
\begin_inset Formula $\phi(x^{(i)})^{T}\phi(x^{(j)})$
\end_inset

, so 
\begin_inset Formula $\Phi\Phi^{T}$
\end_inset

 is the Kernel matrix, 
\begin_inset Formula $K$
\end_inset

.
\end_layout

\begin_layout Standard
For a new observation 
\begin_inset Formula $x_{new}$
\end_inset

, the prediction is given by
\begin_inset Formula 
\begin{align*}
y_{new} & =\theta^{T}\phi(x_{new})\\
 & =Y^{T}(\lambda I+K)^{-1}\Phi\phi(x_{new}).
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We only need to rewrite the expression 
\begin_inset Formula $\Phi\phi(x_{new})$
\end_inset

 in terms of the kernel function.
 To do so, note that 
\begin_inset Formula $i$
\end_inset

th entry of 
\begin_inset Formula $\Phi\phi(x_{new})$
\end_inset

 is 
\begin_inset Formula $\phi(x^{(i)})^{T}\phi(x_{new})=K(\phi(x^{(i)}),\phi(x_{new}))$
\end_inset

.
 Finally, we can use the assumption that, for some 
\begin_inset Formula $\alpha$
\end_inset

, 
\begin_inset Formula $\theta=\sum_{i=1}^{m}\alpha_{i}\phi(x^{(i)})=\Phi^{T}\alpha$
\end_inset

, so 
\begin_inset Formula $\theta^{T}=\alpha^{T}\Phi$
\end_inset

.
 In our case, 
\begin_inset Formula $\alpha^{T}=Y^{T}(\lambda I+K)^{-1}$
\end_inset

.
 Combining, we get 
\begin_inset Formula 
\[
y_{new}=\sum_{i=1}^{m}\alpha_{i}K(x^{(i)},x_{new}).
\]

\end_inset


\end_layout

\begin_layout Standard
All terms in the sum are calculated in terms of 
\begin_inset Formula $K$
\end_inset

, so we're done.
\end_layout

\end_deeper
\begin_layout Section
\begin_inset Formula $\ell_{2}$
\end_inset

 norm soft margin SVMs
\end_layout

\begin_layout Enumerate
Permitting negative numbers does not affect the objective function, and
 the feasibility space corresponding to negative numbers is a strict subset
 of the space corresponding to positive numbers.
\end_layout

\begin_layout Enumerate
The Lagrangian is
\begin_inset Formula 
\[
\mathcal{L}(w,b,\alpha,\xi)=\frac{1}{2}\left\Vert w\right\Vert ^{2}+\frac{C}{2}\sum_{i=1}^{m}\xi_{i}^{2}+\sum_{i=1}^{m}\alpha_{i}\left[-y^{(i)}(w^{T}x^{(i)}+b)+1-\xi_{i}\right].
\]

\end_inset


\end_layout

\begin_layout Enumerate
Taking partials with respect to 
\begin_inset Formula $w,b$
\end_inset

 and 
\begin_inset Formula $\xi$
\end_inset

, and setting to zero, we get
\begin_inset Formula 
\[
\nabla_{w}\mathcal{L}=w-\sum_{i=1}^{m}\alpha_{i}y^{(i)}x^{(i)}=0\Longrightarrow w=\sum_{i=1}^{m}\alpha_{i}y^{(i)}x^{(i)}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial\mathcal{L}}{\partial b}=\sum_{i=1}^{m}\alpha_{i}y^{(i)}=0
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{\xi}\mathcal{L}=C\xi-\alpha\Longrightarrow C\xi=\alpha
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
We want to use the relationships above to rewrite 
\begin_inset Formula $\mathcal{L}$
\end_inset

 as a function of 
\begin_inset Formula $\alpha$
\end_inset

.
 Starting with 
\begin_inset Formula $\frac{1}{2}\left\Vert w\right\Vert ^{2}$
\end_inset

, we have
\begin_inset Formula 
\begin{align*}
\frac{1}{2}\left\Vert w\right\Vert ^{2} & =\frac{1}{2}\sum_{i=1}^{m}\alpha_{i}y^{(i)}x^{(i)}\sum_{j=1}^{m}\alpha_{j}y^{(j)}x^{(j)}\\
 & =\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y^{(i)}y^{(j)}x^{(i)}x^{(j)}
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Substituing the formulas for 
\begin_inset Formula $w$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

into the two right-most term, we get
\begin_inset Formula 
\begin{align*}
\frac{C}{2}\sum_{i=1}^{m}\xi_{i}^{2}+\sum_{i=1}^{m}\alpha_{i}\left[-y^{(i)}(\sum_{j=1}^{m}\alpha_{j}y^{(j)}x^{(j)}x^{(i)}+b)+1-\xi_{i}\right]\\
=-\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y^{(i)}y^{(j)}x^{(i)}x^{(j)}+\sum_{i=1}^{m}\alpha_{i}-\frac{1}{C}\sum_{i=1}^{m}\alpha_{i}^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Combining these results, the dual problem is to maximize
\begin_inset Formula 
\[
-\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y^{(i)}y^{(j)}x^{(i)}x^{(j)}+\sum_{i=1}^{m}\alpha_{i}-\frac{1}{C}\sum_{i=1}^{m}\alpha_{i}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
with respect to 
\begin_inset Formula $\alpha$
\end_inset

, such that 
\begin_inset Formula $\sum_{i=1}^{m}\alpha_{i}y^{(i)}=0$
\end_inset

.s
\end_layout

\end_deeper
\begin_layout Section
SVM with Gaussian kernel
\end_layout

\begin_layout Enumerate
Taking the provided hint and running with it, we've got
\begin_inset Formula 
\begin{align*}
\left|f(x^{(j)})-y^{(j)}\right| & =\left|\sum_{i=1}^{m}y^{(i)}K(x^{(i)},x^{(j)})-y^{(j)}\right|\\
 & =\left|\sum_{i\neq j}^{m}y^{(i)}K(x^{(i)},x^{(j)})\right|+\left|y^{(j)}-y^{(j)}\right|\\
 & =\left|\sum_{i\neq j}^{m}y^{(i)}K(x^{(i)},x^{(j)})\right|\\
 & \leq\left|\sum_{i\neq j}^{m}y^{(i)}e^{-\frac{z^{2}}{\tau^{2}}}\right|\\
 & \leq\sum_{i\neq j}^{m}\left|y^{(i)}e^{-\frac{z^{2}}{\tau^{2}}}\right|\\
 & =\sum_{i\neq j}^{m}e^{-\frac{z^{2}}{\tau^{2}}}=(m-1)e^{-\frac{z^{2}}{\tau^{2}}}<1
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Rearranging the last inequality, we get 
\begin_inset Formula $\tau>\frac{z}{\sqrt{ln(m-1)}}.$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Yes, by design, the resulting classifier will achieve zero training error,
 though not necessarily zero test error.
\end_layout

\begin_layout Enumerate
No, the parameter 
\begin_inset Formula $C$
\end_inset

 regulates the trade-off between bias and variance, or training and test
 error.
 When 
\begin_inset Formula $C$
\end_inset

 is small, the objective function may obtain a minimum when 
\begin_inset Formula $w$
\end_inset

 is small but 
\begin_inset Formula $\xi_{i}$
\end_inset

 terms are potentially large.
 If the latter are large, the model could have non-zero training error.
\end_layout

\begin_layout Section
Naive Bayes and SVMs for Spam Classification
\end_layout

\begin_layout Standard
SVMs outperform Naive Bayes on small sample szies, but Naive Bayes has a
 lower generalization error for sample sizes greater than 1000.
 See attached graphs.
 
\end_layout

\begin_layout Section
Uniform Convergence
\end_layout

\begin_layout Enumerate
With generalization error Î³, the probability of not making an error is 1-
\begin_inset Formula $\gamma$
\end_inset

, and the probability of making no errors on 
\begin_inset Formula $m$
\end_inset

 examples is 
\begin_inset Formula $(1-\gamma)^{m}$
\end_inset

.
 Using the hint, that 
\begin_inset Formula $\left(1-\gamma\right)^{m}\leq e^{-\gamma m}$
\end_inset

, we set
\begin_inset Formula 
\begin{align*}
1-k\exp\left(-\gamma m\right) & =1-\delta\\
\exp\left(-\gamma m\right) & =\frac{\delta}{k}\\
\gamma & =\frac{1}{m}\ln\left(\frac{\delta}{k}\right)\\
\gamma & =\frac{1}{m}\ln\left(\frac{k}{\delta}\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Rearranging for 
\begin_inset Formula $m$
\end_inset

, we get 
\begin_inset Formula $m=\frac{1}{\gamma}\ln\left(\frac{k}{\delta}\right)$
\end_inset

.
\end_layout

\end_body
\end_document
